# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fzf5lnVURnIY01c-iptQ9Okb1C9qxVh7
"""

# LINEAR data preps
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import os

# Set visualization style
plt.style.use('seaborn-v0_8')
sns.set_theme(style="whitegrid")

def save_file(df, filename):
    """Save DataFrame to CSV"""
    df.to_csv(filename, index=False)
    print(f"Saved: {filename}")

# Load data
try:
    df = pd.read_csv('cleaned_tourism_data.csv')
except FileNotFoundError:
    raise FileNotFoundError("Data file not found")

# Select only needed features
features = ['tourism_arrivals', 'tourism_expenditures', 'gdp']
df = df[features].copy()

# Data validation
print("\nData Summary:")
print(f"Rows: {len(df)}")
print("Missing values:")
print(df.isnull().sum())

# Save original data (no transformations)
save_file(df, 'linear_transformed_data.csv')  # Changed filename

# Prepare features and target (using original columns)
X = df[['tourism_arrivals', 'tourism_expenditures']]  # No log_ prefix
y = df['gdp']  # Original target

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Scale features (only linear standardization)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Save processed data
print("\nSaving processed data:")
save_file(pd.DataFrame(X_train_scaled, columns=X.columns), 'X_train_scaled.csv')
save_file(pd.DataFrame(X_test_scaled, columns=X.columns), 'X_test_scaled.csv')
save_file(y_train.to_frame(), 'y_train.csv')
save_file(y_test.to_frame(), 'y_test.csv')

# Visualization (original scale)
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
sns.scatterplot(x='tourism_arrivals', y='gdp', data=df)
plt.title('Arrivals vs GDP (Linear)')

plt.subplot(1, 2, 2)
sns.scatterplot(x='tourism_expenditures', y='gdp', data=df)
plt.title('Expenditures vs GDP (Linear)')

plt.tight_layout()
plt.savefig('linear_feature_relationships.png')
plt.close()

print("\nProcess completed successfully!")
print(f"\nFinal dataset sizes:")
print(f"Training: {len(X_train)} samples")
print(f"Testing: {len(X_test)} samples")

# SVM model with  linear-scaled data  transforms
import pandas as pd
import os
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
import joblib
import json
from datetime import datetime
import matplotlib.pyplot as plt
import numpy as np

# 1. Load and verify ORIGINAL (non-logged) data
required_files = ['X_train_scaled.csv', 'X_test_scaled.csv', 'y_train.csv', 'y_test.csv']
missing_files = [f for f in required_files if not os.path.exists(f)]
if missing_files:
    raise FileNotFoundError(f"Missing required files: {missing_files}")

try:
    X_train = pd.read_csv('X_train_scaled.csv')
    X_test = pd.read_csv('X_test_scaled.csv')
    y_train = pd.read_csv('y_train.csv').squeeze()
    y_test = pd.read_csv('y_test.csv').squeeze()

    print("Data loaded successfully!")
    print(f"Train shapes - X: {X_train.shape}, y: {y_train.shape}")
    print(f"Test shapes - X: {X_test.shape}, y: {y_test.shape}")
    print("\nFeatures being used:", X_train.columns.tolist())

    # Verify no log-transformed columns are present
    if any(col.startswith('log_') for col in X_train.columns):
        raise ValueError("Data appears to contain log-transformed features. Use original features.")

except Exception as e:
    print(f"Error loading data: {e}")
    raise

# 2. Train SVM model (now truly linear-scaled data)
print("\nTraining SVM model on linear-scaled data...")
svm_model = SVR(
    kernel='rbf',        # Still RBF, but works better with standardized linear data
    C=1.0,              # Regularization parameter
    epsilon=0.1,        # Epsilon in epsilon-SVR model
    gamma='scale'       # Kernel coefficient
)
svm_model.fit(X_train, y_train)
print("Training completed!")

# 3. Evaluate model (no expm1 conversions needed)
test_pred = svm_model.predict(X_test)
train_pred = svm_model.predict(X_train)

# Metrics in original linear scale
test_mae = mean_absolute_error(y_test, test_pred)
test_r2 = r2_score(y_test, test_pred)
test_mse = mean_squared_error(y_test, test_pred)
train_r2 = r2_score(y_train, train_pred)

print("\n=== Model Evaluation (Linear Scale) ===")
print(f"Test Set MAE: {test_mae:.4f}")
print(f"Test Set MSE: {test_mse:.4f}")
print(f"Test Set R²: {test_r2:.4f}")
print(f"Train Set R²: {train_r2:.4f}")

# 4. Visualization (no log-original conversion)
plt.figure(figsize=(12, 6))
plt.scatter(y_test, test_pred, alpha=0.6)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.xlabel('Actual GDP ($)')
plt.ylabel('Predicted GDP ($)')
plt.title('SVM Predictions (Linear Data)')
plt.grid(True)
plt.tight_layout()
plt.savefig('svm_linear_predictions.png')
plt.show()

# 5. Save model and metadata
model_filename = 'svm_linear_gdp_predictor.joblib'
joblib.dump(svm_model, model_filename)

metadata = {
    'model_type': 'SVR (Linear Data)',
    'trained_date': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
    'features_used': list(X_train.columns),
    'hyperparameters': {
        'kernel': 'rbf',
        'C': 1.0,
        'epsilon': 0.1,
        'gamma': 'scale'
    },
    'performance_metrics': {
        'test_MAE': float(test_mae),
        'test_MSE': float(test_mse),
        'test_R2': float(test_r2),
        'train_R2': float(train_r2)
    }
}

with open('svm_linear_model_metadata.json', 'w') as f:
    json.dump(metadata, f, indent=4)

print(f"\nModel saved as {model_filename}")
print("Metadata saved as svm_linear_model_metadata.json")

# RANDOM FOREST MODEL - LINEAR DATA
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
import joblib
import json
from datetime import datetime

# Load data
try:
    X_train = pd.read_csv('X_train_scaled.csv')
    X_test = pd.read_csv('X_test_scaled.csv')
    y_train = pd.read_csv('y_train.csv').squeeze()
    y_test = pd.read_csv('y_test.csv').squeeze()
    print(f"Data loaded. Train: {len(X_train)}, Test: {len(X_test)}")
except Exception as e:
    print(f"Error: {e}")
    raise

# Train model
rf_model = RandomForestRegressor(
    n_estimators=200,
    max_depth=10,
    min_samples_split=5,
    random_state=42,
    n_jobs=-1
)
rf_model.fit(X_train, y_train)

# Predict and evaluate
test_pred = rf_model.predict(X_test)
metrics = {
    'Test MAE': f"${mean_absolute_error(y_test, test_pred):,.2f}",
    'Test RMSE': f"${np.sqrt(mean_squared_error(y_test, test_pred)):,.2f}",
    'Test R²': r2_score(y_test, test_pred)
}
print("\nPerformance:", metrics)

# Feature importance
feature_imp = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': rf_model.feature_importances_
}).sort_values('Importance', ascending=False)
print("\nFeature Importance:\n", feature_imp)

# Visualizations
plt.figure(figsize=(10,5))
sns.barplot(x='Importance', y='Feature', data=feature_imp)
plt.title('Feature Importance')
plt.tight_layout()
plt.savefig('rf_feature_importance.png')

plt.figure(figsize=(10,6))
plt.scatter(y_test, test_pred, alpha=0.6)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.xlabel('Actual GDP ($)')
plt.ylabel('Predicted GDP ($)')
plt.text(0.05, 0.9, f"MAE: {metrics['Test MAE']}\nR²: {metrics['Test R²']:.3f}",
         transform=plt.gca().transAxes, bbox=dict(facecolor='white', alpha=0.8))
plt.grid(True)
plt.savefig('rf_predictions.png')

# Save artifacts
joblib.dump(rf_model, 'rf_gdp_predictor.joblib')
with open('rf_model_metadata.json', 'w') as f:
    json.dump({
        'model_type': 'RandomForest',
        'trained_date': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        'features_used': list(X_train.columns),
        'hyperparameters': {
            'n_estimators': 200,
            'max_depth': 10,
            'min_samples_split': 5
        },
        'performance_metrics': metrics,
        'feature_importances': feature_imp.to_dict('records')
    }, f, indent=4)

print("\nSaved: model, plots, metadata")

