# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CMhmC_5DNfO4EfmGshJV8JqOov7bogID
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import os

# Set visualization style
plt.style.use('seaborn-v0_8')
sns.set_theme(style="whitegrid")

def save_file(df, filename):
    """Save DataFrame to CSV"""
    df.to_csv(filename, index=False)
    print(f"Saved: {filename}")

# Load data
try:
    df = pd.read_csv('cleaned_tourism_data.csv')
except FileNotFoundError:
    raise FileNotFoundError("Data file not found")

# Select only needed features
features = ['tourism_arrivals', 'tourism_expenditures', 'gdp']
df = df[features].copy()

# Data validation
print("\nData Summary:")
print(f"Rows: {len(df)}")
print("Missing values:")
print(df.isnull().sum())

# Apply log transformation with shift for non-positive values
print("\nApplying log transformations:")
for col in features:
    min_val = df[col].min()
    if min_val <= 0:
        shift = abs(min_val) + 1
        df[f'log_{col}'] = np.log1p(df[col] + shift)
        print(f"Shifted {col} by {shift:.2f} for log transform")
    else:
        df[f'log_{col}'] = np.log1p(df[col])

# Save transformed data
save_file(df, 'log_transformed_data.csv')

# Prepare features and target
X = df[['log_tourism_arrivals', 'log_tourism_expenditures']]
y = df['log_gdp']

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Save processed data
print("\nSaving processed data:")
save_file(pd.DataFrame(X_train_scaled, columns=X.columns), 'X_train_scaled.csv')
save_file(pd.DataFrame(X_test_scaled, columns=X.columns), 'X_test_scaled.csv')
save_file(y_train.to_frame(), 'y_train.csv')
save_file(y_test.to_frame(), 'y_test.csv')

# Visualization
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
sns.scatterplot(x='log_tourism_arrivals', y='log_gdp', data=df)
plt.title('Arrivals vs GDP')

plt.subplot(1, 2, 2)
sns.scatterplot(x='log_tourism_expenditures', y='log_gdp', data=df)
plt.title('Expenditures vs GDP')

plt.tight_layout()
plt.savefig('feature_relationships.png')
plt.close()

print("\nProcess completed successfully!")
print(f"\nFinal dataset sizes:")
print(f"Training: {len(X_train)} samples")
print(f"Testing: {len(X_test)} samples")

# random forest.py- model with unscaled data

import pandas as pd
import os
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
import joblib
import json
from datetime import datetime
import matplotlib.pyplot as plt

# 1. Load scaled datasets
required_files = ['X_train_scaled.csv', 'X_test_scaled.csv', 'y_train.csv', 'y_test.csv']
missing_files = [f for f in required_files if not os.path.exists(f)]
if missing_files:
    raise FileNotFoundError(f"Missing required files: {missing_files}")

try:
    X_train = pd.read_csv('X_train_scaled.csv')
    X_test = pd.read_csv('X_test_scaled.csv')
    y_train = pd.read_csv('y_train.csv')
    y_test = pd.read_csv('y_test.csv')

    y_train = y_train.squeeze()
    y_test = y_test.squeeze()

    print("Data loaded successfully!")
    print(f"Train shapes - X: {X_train.shape}, y: {y_train.shape}")
    print(f"Test shapes - X: {X_test.shape}, y: {y_test.shape}")

    # Show feature names
    print("\nFeatures being used:")
    print(X_train.columns.tolist())

except Exception as e:
    print(f"Error loading data: {e}")
    raise

# 2. Train Random Forest model
print("\nTraining Random Forest model...")
rf_model = RandomForestRegressor(
    n_estimators=220,    # Number of trees in the forest
    max_depth=7,         # Maximum depth of each tree
    random_state=42,     # For reproducibility
    n_jobs=-1           # Use all available cores
)
rf_model.fit(X_train, y_train)
print("Training completed!")

# 3. Evaluate model
test_pred = rf_model.predict(X_test)
train_pred = rf_model.predict(X_train)  # For checking overfitting

# Calculate metrics
test_mae = mean_absolute_error(y_test, test_pred)
test_r2 = r2_score(y_test, test_pred)
test_mse = mean_squared_error(y_test, test_pred)
train_r2 = r2_score(y_train, train_pred)

print("\n=== Model Evaluation ===")
print(f"Test Set MAE: {test_mae:.2f}")
print(f"Test Set MSE: {test_mse:.2f}")
print(f"Test Set R²: {test_r2:.4f}")
print(f"Train Set R²: {train_r2:.4f} (check for overfitting)")

# 4. Feature importance (only 2 features in this case)
importance_df = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': rf_model.feature_importances_
}).sort_values('Importance', ascending=False)

print("\nFeature Importance:")
print(importance_df)

# 5. Visualization
plt.figure(figsize=(10, 6))
plt.scatter(y_test, test_pred, alpha=0.6)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.xlabel('Actual log(GDP)')
plt.ylabel('Predicted log(GDP)')
plt.title('Actual vs Predicted GDP (Test Set)')
plt.grid(True)
plt.savefig('actual_vs_predicted.png')
plt.show()

# 6. Save model and metadata
model_filename = 'random_forest_model.joblib'
joblib.dump(rf_model, model_filename)

metadata = {
    'model_type': 'RandomForestRegressor',
    'trained_date': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
    'features_used': list(X_train.columns),
    'hyperparameters': {
        'n_estimators': 220,
        'max_depth': 7,
        'random_state': 42
    },
    'performance_metrics': {
        'test_MAE': float(test_mae),
        'test_MSE': float(test_mse),
        'test_R2': float(test_r2),
        'train_R2': float(train_r2)
    },
    'feature_importance': importance_df.to_dict(orient='records')
}

with open('model_metadata.json', 'w') as f:
    json.dump(metadata, f, indent=4)

print(f"\nModel saved as {model_filename}")
print("Metadata saved as model_metadata.json")


# with manually selced features
import pandas as pd
import os
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
import joblib
import json
from datetime import datetime
import matplotlib.pyplot as plt

# 1. Load and verify data
required_files = ['X_train.csv', 'X_test.csv', 'y_train.csv', 'y_test.csv']
missing_files = [f for f in required_files if not os.path.exists(f)]
if missing_files:
    raise FileNotFoundError(f"Missing required files: {missing_files}")

try:
    X_train = pd.read_csv('X_train.csv')
    X_test = pd.read_csv('X_test.csv')
    y_train = pd.read_csv('y_train.csv')
    y_test = pd.read_csv('y_test.csv')

    y_train = y_train.squeeze()
    y_test = y_test.squeeze()

    print("Data loaded successfully!")
    print(f"Train shapes - X: {X_train.shape}, y: {y_train.shape}")
    print(f"Test shapes - X: {X_test.shape}, y: {y_test.shape}")

except Exception as e:
    print(f"Error loading data: {e}")
    raise

# 2. Train Random Forest model
print("\nTraining Random Forest model...")
rf_model = RandomForestRegressor(
    n_estimators=220,    # Number of trees in the forest
    max_depth=7,         # Maximum depth of each tree
    random_state=42,     # For reproducibility
    n_jobs=-1           # Use all available cores
)
rf_model.fit(X_train, y_train)
print("Training completed!")

# 3. Evaluate model
test_pred = rf_model.predict(X_test)
train_pred = rf_model.predict(X_train)  # For checking overfitting

# Calculate metrics
test_mae = mean_absolute_error(y_test, test_pred)
test_r2 = r2_score(y_test, test_pred)
test_mse = mean_squared_error(y_test, test_pred)
train_r2 = r2_score(y_train, train_pred)

print("\n=== Model Evaluation ===")
print(f"Test Set MAE: {test_mae:.2f}")
print(f"Test Set MSE: {test_mse:.2f}")
print(f"Test Set R²: {test_r2:.4f}")
print(f"Train Set R²: {train_r2:.4f} (check for overfitting)")

# 4. Feature importance
importance_df = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': rf_model.feature_importances_
}).sort_values('Importance', ascending=False)

print("\nTop 10 Features:")
print(importance_df.head(10))

# 5. Visualization
plt.figure(figsize=(10, 6))
plt.scatter(y_test, test_pred, alpha=0.6)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs Predicted Values (Test Set)')
plt.grid(True)
plt.savefig('actual_vs_predicted.png')
plt.show()

# 6. Save model and metadata
model_filename = 'random_forest_model.joblib'
joblib.dump(rf_model, model_filename)

metadata = {
    'model_type': 'RandomForestRegressor',
    'trained_date': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
    'features_used': list(X_train.columns),
    'hyperparameters': {
        'n_estimators': 150,
        'max_depth': 7,
        'random_state': 42
    },
    'performance_metrics': {
        'test_MAE': float(test_mae),
        'test_MSE': float(test_mse),
        'test_R2': float(test_r2),
        'train_R2': float(train_r2)
    },
    'top_features': importance_df.head(10).to_dict(orient='records')
}

with open('model_metadata.json', 'w') as f:
    json.dump(metadata, f, indent=4)

print(f"\nModel saved as {model_filename}")
print("Metadata saved as model_metadata.json")

#SVM MODELS WITH UNSCALED DATA
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
import joblib
import json
from datetime import datetime
import matplotlib.pyplot as plt
import numpy as np

# 1. Load your pre-split data
X_train = pd.read_csv('X_train.csv')
X_test = pd.read_csv('X_test.csv')
y_train = pd.read_csv('y_train.csv')
y_test = pd.read_csv('y_test.csv')

# y_train and y_test in 1D arrays (required by scikit-learn)
y_train = y_train.squeeze()
y_test = y_test.squeeze()

# (Optional) Verify shapes
print("Train shapes - X:", X_train.shape, "y:", y_train.shape)
print("Test shapes - X:", X_test.shape, "y:", y_test.shape)

# 2. Train the Random Forest model
rf_model = RandomForestRegressor(
    n_estimators=150,
    max_depth=7,
    random_state=42
)
rf_model.fit(X_train, y_train)

# 3. Evaluate on test set
test_pred = rf_model.predict(X_test)

# Calculate all metrics
test_mae = mean_absolute_error(y_test, test_pred)
test_mse = mean_squared_error(y_test, test_pred)
test_rmse = np.sqrt(test_mse)
test_r2 = r2_score(y_test, test_pred)

def mean_absolute_percentage_error(y_true, y_pred):
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    mask = y_true != 0
    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100

test_mape = mean_absolute_percentage_error(y_test, test_pred)

# 4. Print performance metrics
print("\n=== Model Performance Metrics ===")
print(f"Mean Absolute Error (MAE): {test_mae:.4f}")
print(f"Mean Squared Error (MSE): {test_mse:.4f}")
print(f"Root Mean Squared Error (RMSE): {test_rmse:.4f}")
print(f"R-squared (R²): {test_r2:.4f}")
print(f"Mean Absolute Percentage Error (MAPE): {test_mape:.4f}%")

# 5. Feature importance
importance_df = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': rf_model.feature_importances_
}).sort_values('Importance', ascending=False)

print("\n=== Feature Importance ===")
print(importance_df.to_string(index=False))

# 6. Plot actual vs predicted
plt.figure(figsize=(10, 6))
plt.scatter(y_test, test_pred, alpha=0.6)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.xlabel('Actual GDP')
plt.ylabel('Predicted GDP')
plt.title('Test Set: Actual vs Predicted GDP')
plt.savefig('actual_vs_predicted.png')
plt.show()

# 7. Save model and metadata
joblib.dump(rf_model, 'rf_gdp_predictor.joblib')

metadata = {
    'model_type': 'RandomForestRegressor',
    'trained_date': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
    'features_used': list(X_train.columns),
    'hyperparameters': {
        'n_estimators': 150,
        'max_depth': 7,
        'random_state': 42
    },
    'performance_metrics': {
        'MAE': float(test_mae),
        'MSE': float(test_mse),
        'RMSE': float(test_rmse),
        'R2': float(test_r2),
        'MAPE': float(test_mape)
    },
    'feature_importance': importance_df.to_dict(orient='records')
}

with open('model_metadata.json', 'w') as f:
    json.dump(metadata, f, indent=4)

print("\nModel and metadata saved successfully.")